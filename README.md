# Pretrained Internet Language Model

## BERT

## RoBERTa

chinese_danmaku_roberta
- link: https://huggingface.co/WUJUNCHAO/chinese_danmaku_roberta
- This model is a fine-tuned version of uer/chinese_roberta_L-8_H-512 on an Danmaku Corpus(2000k raw data) dataset. 
- It achieves the following results on the evaluation set:
  - Loss: 1.1645
  - Accuracy: 0.7780

## T5
